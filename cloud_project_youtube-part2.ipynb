{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78713662d16e46bcab32fcb3b3c72e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1587271808365_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-21-116.ec2.internal:20888/proxy/application_1587271808365_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-26-130.ec2.internal:8042/node/containerlogs/container_1587271808365_0004_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9059c1b616419f860e7537c5ae4224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n",
      "Installing collected packages: pyparsing, python-dateutil, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7 python-dateutil-2.8.1\n",
      "\n",
      "Collecting pandas==0.25.1\n",
      "  Using cached pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from pandas==0.25.1) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==0.25.1) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==0.25.1) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-0.25.1\n",
      "\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.10.0-py3-none-any.whl (215 kB)\n",
      "Collecting scipy>=1.0.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from seaborn) (1.14.5)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from seaborn) (0.25.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.13.0)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.4.1 seaborn-0.10.0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# needs to be set for matplotlib and seaborn\n",
    "os.environ['LANG'] = \"en_US.UTF-8\"\n",
    "os.environ['LC_ALL'] = \"en_US.UTF-8\"\n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "sc.install_pypi_package(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6994b860dff46d08cf4833de1b916fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket='s3://youtube-data-cloud'\n",
    "# file_US='/USvideos.csv'\n",
    "file_path = \"s3://youtube-data-cloud/CAvideos.csv\"\n",
    "# json_file_US='/US_category_id.json'\n",
    "# rdd_US = sc.textFile(bucket+file_US).map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb19505048448539d81facb9f7a3ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefccd74acd14b15b0f00479821c94c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = sqlContext.read.csv(file_path, header=True)\n",
    "# category_data=spark.read.option(\"multiline\",True).json(bucket+json_file_US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afec2be9f584ad990428a314d5807f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "data_df.cache()\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6b46329002443d88e5f898353b097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85430344347448fe9a3989335ddf5a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0  ...                                                  4\n",
      "summary                 count  ...                                                max\n",
      "video_id                45560  ...  â—Žè¿½è¹¤ã€ä¸‰ç«‹æ–°èžç¶²Instagramã€‘è¶…æœ‰æ¢—ï¼š https://www.instagram....\n",
      "trending_date           41537  ...  â€ â€œGorillas in the Mistâ€) and editor Michael T...\n",
      "title                   41383  ...  ðŸš¨Active Shooter at YouTube Headquarters - LIVE...\n",
      "channel_title           41321  ...                                              í™œë ¥ì†ŒTV\n",
      "category_id             41195  ...              Yashma Gill as Erum (Shafaqâ€™s bhabhi)\n",
      "publish_time            41142  ...               Ali Ansari as Aftab (Anjiâ€™s husband)\n",
      "tags                    40980  ...                                              ì´êµ­ì¢…êµìˆ˜\n",
      "views                   40946  ...          Mariyam Mirza as Naheed (Shafaqâ€™s mother)\n",
      "likes                   40938  ...            Shajeer Uddin as Atta (Shafaqâ€™s father)\n",
      "dislikes                40938  ...         Barjees Farooqui as Simi (Shafaq ki khala)\n",
      "comment_count           40935  ...            Mabsira Khan as Shahida (Erumâ€™s mother)\n",
      "thumbnail_link          40935  ...     https://i.ytimg.com/vi/zzjNCiCqiOs/default.jpg\n",
      "comments_disabled       40933  ...                 â€ she added.â€œShe was in great form\n",
      "ratings_disabled        40905  ...                                               True\n",
      "video_error_or_removed  40894  ...                                               True\n",
      "description             39598  ...  ðŸš¨ NEW MERCH! http://amzn.to/annoyingorange ðŸš¨âž¤ ...\n",
      "\n",
      "[17 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "data_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9670024233ac4b4ab0ac2537822b7008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c2bb5ec63946b8abf6d8a9bd4110a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = data_df.withColumn(\"views\", data_df[\"views\"].cast(IntegerType())).withColumn(\"likes\", data_df[\"likes\"].cast(IntegerType())).withColumn(\"dislikes\", data_df[\"dislikes\"].cast(IntegerType())).withColumn(\"comment_count\", data_df[\"comment_count\"].cast(IntegerType())).withColumn(\"trending_date\",data_df[\"trending_date\"].cast(\"timestamp\")).withColumn(\"publish_time\",data_df[\"publish_time\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f428cb11124e9d96104377c941549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('video_id', 'string'), ('trending_date', 'timestamp'), ('title', 'string'), ('channel_title', 'string'), ('category_id', 'string'), ('publish_time', 'timestamp'), ('tags', 'string'), ('views', 'int'), ('likes', 'int'), ('dislikes', 'int'), ('comment_count', 'int'), ('thumbnail_link', 'string'), ('comments_disabled', 'string'), ('ratings_disabled', 'string'), ('video_error_or_removed', 'string'), ('description', 'string')]"
     ]
    }
   ],
   "source": [
    "new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c07420339947248b9428d6655f5b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[video_id: string, trending_date: timestamp, title: string, channel_title: string, category_id: string, publish_time: timestamp, tags: string, views: int, likes: int, dislikes: int, comment_count: int, thumbnail_link: string, comments_disabled: string, ratings_disabled: string, video_error_or_removed: string, description: string]"
     ]
    }
   ],
   "source": [
    "new_data.na.drop()\n",
    "new_data.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2e33b682bc4ecda851b68b4e15fe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_data.fillna(0, subset=['likes', 'dislikes','comment_count'])\n",
    "# new_data.fillna('', subset=['title', 'channel_title','tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8947226844664bccbba961d7f2e85c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                        0  ...                                                  4\n",
      "video_id                                                      n1WpP7iowLc  ...                                        2Vv-BfVoq4g\n",
      "trending_date                                                        None  ...                                               None\n",
      "title                          Eminem - Walk On Water (Audio) ft. BeyoncÃ©  ...        Ed Sheeran - Perfect (Official Music Video)\n",
      "channel_title                                                  EminemVEVO  ...                                         Ed Sheeran\n",
      "category_id                                                            10  ...                                                 10\n",
      "publish_time                                          2017-11-10 17:00:03  ...                                2017-11-09 11:04:14\n",
      "tags                    \"Eminem\"|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/...  ...  \"edsheeran\"|\"ed sheeran\"|\"acoustic\"|\"live\"|\"co...\n",
      "views                                                            17158579  ...                                           33523622\n",
      "likes                                                              787425  ...                                            1634130\n",
      "dislikes                                                            43420  ...                                              21082\n",
      "comment_count                                                      125882  ...                                              85067\n",
      "thumbnail_link             https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg  ...     https://i.ytimg.com/vi/2Vv-BfVoq4g/default.jpg\n",
      "comments_disabled                                                   False  ...                                              False\n",
      "ratings_disabled                                                    False  ...                                              False\n",
      "video_error_or_removed                                              False  ...                                              False\n",
      "description             Eminem's new track Walk on Water ft. BeyoncÃ© i...  ...  ðŸŽ§: https://ad.gt/yt-perfect\\nðŸ’°: https://atlant...\n",
      "\n",
      "[16 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069b3af65f1541f1996d8586cbf48a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0                   1                   2    3          4\n",
      "summary        count                mean              stddev  min        max\n",
      "views          40881  1147035.9107898534  3390913.0223090285  733  137843120\n",
      "likes          40881   39582.68824148137  132689.53073880955    0    5053338\n",
      "dislikes       40881  2009.1954453168953   19008.36979098862    0    1602383\n",
      "comment_count  40881   5042.974707076637     21579.020654127    0    1114800"
     ]
    }
   ],
   "source": [
    "numeric_features = [t[0] for t in new_data.dtypes if t[1] == 'int']\n",
    "new_data.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260e916f3ad94ba98208b4b0c72b95bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]"
     ]
    }
   ],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "numeric_data = new_data.select(numeric_features).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d1d02eb294daf86f35ed318212e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# new_data = new_data.select( 'category_id', 'tags',  'views', 'likes', 'dislikes', 'comment_count', 'description')\n",
    "# new_data = new_data.select('title', 'views', 'likes', 'dislikes', 'comment_count')\n",
    "new_data = new_data.select('video_id', 'category_id', 'views', 'likes', 'dislikes', 'comment_count')\n",
    "# new_data = new_data.select('views', 'likes', 'dislikes', 'comment_count')\n",
    "cols = new_data.columns\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe23334d7244eb98b2439aa427e792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['video_id']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'category_id', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['views', 'likes', 'dislikes', 'comment_count']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1662ffb5363d4ee592bcb992886cb485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# vectorAssembler = VectorAssembler(inputCols = ['views', 'likes', 'dislikes'], outputCol = 'features')\n",
    "# vhouse_df = vectorAssembler.transform(new_data)\n",
    "# vhouse_df = vhouse_df.select(['features', 'comment_count'])\n",
    "# vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7dd46d6f694ebfae92c4cc7d697930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'requirement failed: Output column label already exists.'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 109, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n",
      "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.IllegalArgumentException: 'requirement failed: Output column label already exists.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(new_data)\n",
    "new_data = pipelineModel.transform(new_data)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "new_data = new_data.select(selectedCols)\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d8b6db974946d79158ea8d074acb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               0  ...                                                  4\n",
      "label                                                          4  ...                                                  4\n",
      "features       (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "video_id                                             n1WpP7iowLc  ...                                        2Vv-BfVoq4g\n",
      "category_id                                                   10  ...                                                 10\n",
      "views                                                   17158579  ...                                           33523622\n",
      "likes                                                     787425  ...                                            1634130\n",
      "dislikes                                                   43420  ...                                              21082\n",
      "comment_count                                             125882  ...                                              85067\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be590b0e6c7c4fcbab3ccbeacea59199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = new_data.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd004fb1f944ecda43449d88a03c9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o5599.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 123.0 failed 4 times, most recent failure: Lost task 1.3 in stage 123.0 (TID 500, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o5599.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 123.0 failed 4 times, most recent failure: Lost task 1.3 in stage 123.0 (TID 500, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d97e2a6494faaa9ef066e0d2f1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o3856.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 81.0 failed 4 times, most recent failure: Lost task 1.3 in stage 81.0 (TID 314, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3856.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 81.0 failed 4 times, most recent failure: Lost task 1.3 in stage 81.0 (TID 314, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6a2d6f01742ac9324ace0cebc7945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               0  ...                                                  4\n",
      "label                                                       1202  ...                                               8722\n",
      "features       (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "title                 Eminem - Walk On Water (Audio) ft. BeyoncÃ©  ...        Ed Sheeran - Perfect (Official Music Video)\n",
      "views                                                   17158579  ...                                           33523622\n",
      "likes                                                     787425  ...                                            1634130\n",
      "dislikes                                                   43420  ...                                              21082\n",
      "comment_count                                             125882  ...                                              85067\n",
      "description    Eminem's new track Walk on Water ft. BeyoncÃ© i...  ...  ðŸŽ§: https://ad.gt/yt-perfect\\nðŸ’°: https://atlant...\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bdd7731f574127b3432d2ff51b48e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = new_data.randomSplit([0.7, 0.3], seed = 2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28bae532594c4a90a2bcf103bd1cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"Training Dataset Count: \" + str(train.count()))\n",
    "# print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce5b73cc375483bbebf2471bee8839b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o3050.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 75.0 failed 4 times, most recent failure: Lost task 3.3 in stage 75.0 (TID 265, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3050.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 75.0 failed 4 times, most recent failure: Lost task 3.3 in stage 75.0 (TID 265, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
