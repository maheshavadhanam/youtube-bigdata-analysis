{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78713662d16e46bcab32fcb3b3c72e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1587271808365_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-21-116.ec2.internal:20888/proxy/application_1587271808365_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-26-130.ec2.internal:8042/node/containerlogs/container_1587271808365_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9059c1b616419f860e7537c5ae4224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n",
      "Installing collected packages: pyparsing, python-dateutil, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7 python-dateutil-2.8.1\n",
      "\n",
      "Collecting pandas==0.25.1\n",
      "  Using cached pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from pandas==0.25.1) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==0.25.1) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==0.25.1) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-0.25.1\n",
      "\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.10.0-py3-none-any.whl (215 kB)\n",
      "Collecting scipy>=1.0.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from seaborn) (1.14.5)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from seaborn) (0.25.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1587282269650-0/lib64/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1587282269650-0/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.13.0)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.4.1 seaborn-0.10.0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# needs to be set for matplotlib and seaborn\n",
    "os.environ['LANG'] = \"en_US.UTF-8\"\n",
    "os.environ['LC_ALL'] = \"en_US.UTF-8\"\n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "sc.install_pypi_package(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6994b860dff46d08cf4833de1b916fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket='s3://youtube-data-cloud'\n",
    "# file_US='/USvideos.csv'\n",
    "file_path = \"s3://youtube-data-cloud/CAvideos.csv\"\n",
    "# json_file_US='/US_category_id.json'\n",
    "# rdd_US = sc.textFile(bucket+file_US).map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb19505048448539d81facb9f7a3ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefccd74acd14b15b0f00479821c94c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = sqlContext.read.csv(file_path, header=True)\n",
    "# category_data=spark.read.option(\"multiline\",True).json(bucket+json_file_US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afec2be9f584ad990428a314d5807f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "data_df.cache()\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6b46329002443d88e5f898353b097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85430344347448fe9a3989335ddf5a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0  ...                                                  4\n",
      "summary                 count  ...                                                max\n",
      "video_id                45560  ...  ◎追蹤【三立新聞網Instagram】超有梗： https://www.instagram....\n",
      "trending_date           41537  ...  ” “Gorillas in the Mist”) and editor Michael T...\n",
      "title                   41383  ...  🚨Active Shooter at YouTube Headquarters - LIVE...\n",
      "channel_title           41321  ...                                              활력소TV\n",
      "category_id             41195  ...              Yashma Gill as Erum (Shafaq’s bhabhi)\n",
      "publish_time            41142  ...               Ali Ansari as Aftab (Anji’s husband)\n",
      "tags                    40980  ...                                              이국종교수\n",
      "views                   40946  ...          Mariyam Mirza as Naheed (Shafaq’s mother)\n",
      "likes                   40938  ...            Shajeer Uddin as Atta (Shafaq’s father)\n",
      "dislikes                40938  ...         Barjees Farooqui as Simi (Shafaq ki khala)\n",
      "comment_count           40935  ...            Mabsira Khan as Shahida (Erum’s mother)\n",
      "thumbnail_link          40935  ...     https://i.ytimg.com/vi/zzjNCiCqiOs/default.jpg\n",
      "comments_disabled       40933  ...                 ” she added.“She was in great form\n",
      "ratings_disabled        40905  ...                                               True\n",
      "video_error_or_removed  40894  ...                                               True\n",
      "description             39598  ...  🚨 NEW MERCH! http://amzn.to/annoyingorange 🚨➤ ...\n",
      "\n",
      "[17 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "data_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9670024233ac4b4ab0ac2537822b7008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c2bb5ec63946b8abf6d8a9bd4110a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_data = data_df.withColumn(\"views\", data_df[\"views\"].cast(IntegerType())).withColumn(\"likes\", data_df[\"likes\"].cast(IntegerType())).withColumn(\"dislikes\", data_df[\"dislikes\"].cast(IntegerType())).withColumn(\"comment_count\", data_df[\"comment_count\"].cast(IntegerType())).withColumn(\"trending_date\",data_df[\"trending_date\"].cast(\"timestamp\")).withColumn(\"publish_time\",data_df[\"publish_time\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f428cb11124e9d96104377c941549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('video_id', 'string'), ('trending_date', 'timestamp'), ('title', 'string'), ('channel_title', 'string'), ('category_id', 'string'), ('publish_time', 'timestamp'), ('tags', 'string'), ('views', 'int'), ('likes', 'int'), ('dislikes', 'int'), ('comment_count', 'int'), ('thumbnail_link', 'string'), ('comments_disabled', 'string'), ('ratings_disabled', 'string'), ('video_error_or_removed', 'string'), ('description', 'string')]"
     ]
    }
   ],
   "source": [
    "new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c07420339947248b9428d6655f5b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[video_id: string, trending_date: timestamp, title: string, channel_title: string, category_id: string, publish_time: timestamp, tags: string, views: int, likes: int, dislikes: int, comment_count: int, thumbnail_link: string, comments_disabled: string, ratings_disabled: string, video_error_or_removed: string, description: string]"
     ]
    }
   ],
   "source": [
    "new_data.na.drop()\n",
    "new_data.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2e33b682bc4ecda851b68b4e15fe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_data.fillna(0, subset=['likes', 'dislikes','comment_count'])\n",
    "# new_data.fillna('', subset=['title', 'channel_title','tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8947226844664bccbba961d7f2e85c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                        0  ...                                                  4\n",
      "video_id                                                      n1WpP7iowLc  ...                                        2Vv-BfVoq4g\n",
      "trending_date                                                        None  ...                                               None\n",
      "title                          Eminem - Walk On Water (Audio) ft. Beyoncé  ...        Ed Sheeran - Perfect (Official Music Video)\n",
      "channel_title                                                  EminemVEVO  ...                                         Ed Sheeran\n",
      "category_id                                                            10  ...                                                 10\n",
      "publish_time                                          2017-11-10 17:00:03  ...                                2017-11-09 11:04:14\n",
      "tags                    \"Eminem\"|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/...  ...  \"edsheeran\"|\"ed sheeran\"|\"acoustic\"|\"live\"|\"co...\n",
      "views                                                            17158579  ...                                           33523622\n",
      "likes                                                              787425  ...                                            1634130\n",
      "dislikes                                                            43420  ...                                              21082\n",
      "comment_count                                                      125882  ...                                              85067\n",
      "thumbnail_link             https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg  ...     https://i.ytimg.com/vi/2Vv-BfVoq4g/default.jpg\n",
      "comments_disabled                                                   False  ...                                              False\n",
      "ratings_disabled                                                    False  ...                                              False\n",
      "video_error_or_removed                                              False  ...                                              False\n",
      "description             Eminem's new track Walk on Water ft. Beyoncé i...  ...  🎧: https://ad.gt/yt-perfect\\n💰: https://atlant...\n",
      "\n",
      "[16 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069b3af65f1541f1996d8586cbf48a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0                   1                   2    3          4\n",
      "summary        count                mean              stddev  min        max\n",
      "views          40881  1147035.9107898534  3390913.0223090285  733  137843120\n",
      "likes          40881   39582.68824148137  132689.53073880955    0    5053338\n",
      "dislikes       40881  2009.1954453168953   19008.36979098862    0    1602383\n",
      "comment_count  40881   5042.974707076637     21579.020654127    0    1114800"
     ]
    }
   ],
   "source": [
    "numeric_features = [t[0] for t in new_data.dtypes if t[1] == 'int']\n",
    "new_data.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260e916f3ad94ba98208b4b0c72b95bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]"
     ]
    }
   ],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "numeric_data = new_data.select(numeric_features).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d1d02eb294daf86f35ed318212e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# new_data = new_data.select( 'category_id', 'tags',  'views', 'likes', 'dislikes', 'comment_count', 'description')\n",
    "# new_data = new_data.select('title', 'views', 'likes', 'dislikes', 'comment_count')\n",
    "new_data = new_data.select('video_id', 'category_id', 'views', 'likes', 'dislikes', 'comment_count')\n",
    "# new_data = new_data.select('views', 'likes', 'dislikes', 'comment_count')\n",
    "cols = new_data.columns\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe23334d7244eb98b2439aa427e792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['video_id']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'category_id', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['views', 'likes', 'dislikes', 'comment_count']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1662ffb5363d4ee592bcb992886cb485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# vectorAssembler = VectorAssembler(inputCols = ['views', 'likes', 'dislikes'], outputCol = 'features')\n",
    "# vhouse_df = vectorAssembler.transform(new_data)\n",
    "# vhouse_df = vhouse_df.select(['features', 'comment_count'])\n",
    "# vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7dd46d6f694ebfae92c4cc7d697930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'requirement failed: Output column label already exists.'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 109, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n",
      "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.IllegalArgumentException: 'requirement failed: Output column label already exists.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(new_data)\n",
    "new_data = pipelineModel.transform(new_data)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "new_data = new_data.select(selectedCols)\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d8b6db974946d79158ea8d074acb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               0  ...                                                  4\n",
      "label                                                          4  ...                                                  4\n",
      "features       (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "video_id                                             n1WpP7iowLc  ...                                        2Vv-BfVoq4g\n",
      "category_id                                                   10  ...                                                 10\n",
      "views                                                   17158579  ...                                           33523622\n",
      "likes                                                     787425  ...                                            1634130\n",
      "dislikes                                                   43420  ...                                              21082\n",
      "comment_count                                             125882  ...                                              85067\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be590b0e6c7c4fcbab3ccbeacea59199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = new_data.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd004fb1f944ecda43449d88a03c9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o5599.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 123.0 failed 4 times, most recent failure: Lost task 1.3 in stage 123.0 (TID 500, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o5599.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 123.0 failed 4 times, most recent failure: Lost task 1.3 in stage 123.0 (TID 500, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d97e2a6494faaa9ef066e0d2f1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o3856.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 81.0 failed 4 times, most recent failure: Lost task 1.3 in stage 81.0 (TID 314, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3856.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 81.0 failed 4 times, most recent failure: Lost task 1.3 in stage 81.0 (TID 314, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<views_double_VectorAssembler_bcb9a32b7a08:double,likes_double_VectorAssembler_bcb9a32b7a08:double,dislikes_double_VectorAssembler_bcb9a32b7a08:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6a2d6f01742ac9324ace0cebc7945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               0  ...                                                  4\n",
      "label                                                       1202  ...                                               8722\n",
      "features       (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "title                 Eminem - Walk On Water (Audio) ft. Beyoncé  ...        Ed Sheeran - Perfect (Official Music Video)\n",
      "views                                                   17158579  ...                                           33523622\n",
      "likes                                                     787425  ...                                            1634130\n",
      "dislikes                                                   43420  ...                                              21082\n",
      "comment_count                                             125882  ...                                              85067\n",
      "description    Eminem's new track Walk on Water ft. Beyoncé i...  ...  🎧: https://ad.gt/yt-perfect\\n💰: https://atlant...\n",
      "\n",
      "[8 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(new_data.take(5), columns=new_data.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bdd7731f574127b3432d2ff51b48e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = new_data.randomSplit([0.7, 0.3], seed = 2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28bae532594c4a90a2bcf103bd1cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"Training Dataset Count: \" + str(train.count()))\n",
    "# print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce5b73cc375483bbebf2471bee8839b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o3050.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 75.0 failed 4 times, most recent failure: Lost task 3.3 in stage 75.0 (TID 265, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3050.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 75.0 failed 4 times, most recent failure: Lost task 3.3 in stage 75.0 (TID 265, ip-172-31-28-58.ec2.internal, executor 1): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n",
      "\t... 27 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
